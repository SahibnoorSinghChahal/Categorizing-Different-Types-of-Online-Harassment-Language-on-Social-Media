{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyenchant\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/54/04d88a59efa33fefb88133ceb638cdf754319030c28aadc5a379d82140ed/pyenchant-2.0.0.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\SAHIBN~1\\AppData\\Local\\Temp\\pip-install-_1qmrg3x\\pyenchant\\setup.py\", line 212, in <module>\n",
      "        import enchant\n",
      "      File \"C:\\Users\\SAHIBN~1\\AppData\\Local\\Temp\\pip-install-_1qmrg3x\\pyenchant\\enchant\\__init__.py\", line 92, in <module>\n",
      "        from enchant import _enchant as _e\n",
      "      File \"C:\\Users\\SAHIBN~1\\AppData\\Local\\Temp\\pip-install-_1qmrg3x\\pyenchant\\enchant\\_enchant.py\", line 145, in <module>\n",
      "        raise ImportError(msg)\n",
      "    ImportError: The 'enchant' C library was not found. Please install it via your OS package manager, or use a pre-built binary wheel from PyPI.\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\SAHIBN~1\\AppData\\Local\\Temp\\pip-install-_1qmrg3x\\pyenchant\\\n"
     ]
    }
   ],
   "source": [
    "!pip install pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4e0f3b965d4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfileinput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'enchant'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import enchant\n",
    "import fileinput\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, Input, SpatialDropout1D\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "#Load Dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "#converting into small letters\n",
    "for line in fileinput.input(\"data.csv\",inplace=1):\n",
    "    print(line.lower(),end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(tweet_content).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "for i in range(len(df)):\n",
    "    string = df.tweet_content[i]\n",
    "    english_words = []\n",
    "    for word in string.split():\n",
    "        if d.check(word):\n",
    "            english_words.append(word)\n",
    "            #print=\" \".join(english_words)\n",
    "            vin=\" \".join(english_words)\n",
    "    df.tweet_content[i]=vin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(untokenized):\n",
    "    tokenized = sent_tokenize(untokenized)\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "def shuffle_tokenize(test):\n",
    "    random.shuffle(text)\n",
    "    newl=list(text)\n",
    "    shuffled.append(newl)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "augmented = []\n",
    "reps = []\n",
    "\n",
    "for neg in df.tweet_content:\n",
    "    if df.harassment == '1' or df.harassment == 1:\n",
    "        txt = tokenize(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following process is an alternative to One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138, 24, 90, 485, 122, 558, 309, 7, 270, 44, 648, 68], [8, 193, 4168, 427, 14, 2, 4169, 63, 56, 169, 6, 1782, 11]]\n"
     ]
    }
   ],
   "source": [
    "all_text2 = ' '.join(df.tweet_content)\n",
    "# create a list of words\n",
    "words = all_text2.split()\n",
    "\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "\n",
    "# Assigning a unique index value to all the words\n",
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "tweets_int = []\n",
    "for tweet in df.tweet_content:\n",
    "    r = [vocab_to_int[w] for w in tweet.split()]\n",
    "    tweets_int.append(r)\n",
    "print (tweets_int[0:2])\n",
    "\n",
    "tweets_len = [len(x) for x in tweets_int]\n",
    "tweets_int = [ tweets_int[i] for i, l in enumerate(tweets_len) if l>0 ]\n",
    "\n",
    "x1=pad_sequences(tweets_int, maxlen=22, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, None, 256)         4404992   \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, None, 128)         32896     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,487,361\n",
      "Trainable params: 4,487,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "y = df.iloc[:,2].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x1, y, test_size=0.1, random_state=42)\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(17207, 256))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, dropout=0.9, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5736 samples, validate on 638 samples\n",
      "Epoch 1/10\n",
      "5736/5736 [==============================] - 29s 5ms/step - loss: 0.6800 - acc: 0.5757 - val_loss: 0.5438 - val_acc: 0.7555\n",
      "Epoch 2/10\n",
      "5736/5736 [==============================] - 27s 5ms/step - loss: 0.4964 - acc: 0.7831 - val_loss: 0.3690 - val_acc: 0.8636\n",
      "Epoch 3/10\n",
      "5736/5736 [==============================] - 20s 3ms/step - loss: 0.3714 - acc: 0.8567 - val_loss: 0.3329 - val_acc: 0.8824\n",
      "Epoch 4/10\n",
      "5736/5736 [==============================] - 20s 3ms/step - loss: 0.3098 - acc: 0.8882 - val_loss: 0.3675 - val_acc: 0.8777\n",
      "Epoch 5/10\n",
      "5736/5736 [==============================] - 21s 4ms/step - loss: 0.2651 - acc: 0.9059 - val_loss: 0.3843 - val_acc: 0.8668\n",
      "Epoch 6/10\n",
      "5736/5736 [==============================] - 21s 4ms/step - loss: 0.2570 - acc: 0.9067 - val_loss: 0.4058 - val_acc: 0.8621\n",
      "Epoch 7/10\n",
      "5736/5736 [==============================] - 21s 4ms/step - loss: 0.2311 - acc: 0.9195 - val_loss: 0.3899 - val_acc: 0.8605\n",
      "Epoch 8/10\n",
      "5736/5736 [==============================] - 21s 4ms/step - loss: 0.2329 - acc: 0.9160 - val_loss: 0.4637 - val_acc: 0.8433\n",
      "Epoch 9/10\n",
      "5736/5736 [==============================] - 20s 4ms/step - loss: 0.2148 - acc: 0.9250 - val_loss: 0.4496 - val_acc: 0.8480\n",
      "Epoch 10/10\n",
      "5736/5736 [==============================] - 21s 4ms/step - loss: 0.2105 - acc: 0.9278 - val_loss: 0.5156 - val_acc: 0.8370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5f1b06e80>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.45439859150345424\n",
      "Test accuracy: 0.8150470226909673\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
